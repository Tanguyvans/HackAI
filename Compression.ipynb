{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanguyvans/HackAI/blob/main/Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HqpyiKFTqRH"
      },
      "source": [
        "# **Quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjSR_PqOTqry",
        "outputId": "2cc1894e-ea2c-4536-f7c5-89541a90511c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 18 13:30:44 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKyJuwotVyS3",
        "outputId": "26c39b86-ea46-4561-f4bd-851ba58d3138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from IPython.display import Image\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "assert torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "Y4JsaTVRP_JM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torch2trt\n",
        "#from torch2trt import torch2trt"
      ],
      "metadata": {
        "id": "-lTtqatJpFBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac87c37-61ae-4930-a008-c176b0cdb826"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch2trt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch2trt\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"os1\"]=\"ubuntu1804\"\n",
        "os.environ[\"tag\"]= \"cuda11.0-trt7.2.3.4-ga-20210226\" #@param\n",
        "os.environ[\"version\"]= \"7.2.3-1+cuda11.0\" #@param\n",
        "data_path = '/content/drive/MyDrive/IA/UMONS/Workshop/Compression' #@param\n",
        "os.chdir(data_path)\n"
      ],
      "metadata": {
        "id": "ndb1Cdka5ryD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo dpkg -i nv-tensorrt-repo-{tag}_1-1_amd64.deb\n",
        "!sudo apt-key add /var/nv-tensorrt-repo-{version} libnvonnxparsers7={version} libnvinfer-plugin7={version} libnvonnxparsers-dev={version} libnvinfer-plugin-dev={version} python3-libnvinfer={version}\n",
        "!sudo apt-get install python3-libnvinfer-dev=${version}\n",
        "\n",
        "!git clone https://github.com/NVIDIA-AI-IOT/torch2trt /content/torch2trt\n",
        "%cd /content/torch2trt\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "ONttBvTJQDBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7cXLuuEpSLf",
        "outputId": "708e8ef4-11ce-4bd1-9b6b-af1b76db592d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zKYrxbyDT9R8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "a4618064-c13b-4806-fe45-50cc79d51417"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sys' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-70d12083209f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassif_folder\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'2. Image classification/'\u001b[0m \u001b[0;31m#@param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassification_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclassif_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
          ]
        }
      ],
      "source": [
        "folder = '/content/drive/MyDrive/IA/UMONS/Workshop/' #@param ['/content/drive/MyDrive/Workshop/', '/content/drive/MyDrive/IA/UMONS/Workshop/']\n",
        "\n",
        "#classification_path = f\"{folder}2. Image classification/\" #@param\n",
        "classif_folder= '2. Image classification/' #@param\n",
        "classification_path = folder + classif_folder\n",
        "sys.path.append(classification_path)\n",
        "os.chdir(classification_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor, Compose, Normalize, Resize\n",
        "class ImageFolderCalibDataset():\n",
        "    def __init__(self, root):\n",
        "        self.dataset = ImageFolder(\n",
        "            root=root,\n",
        "            transform=Compose([\n",
        "                Resize((224, 224)),\n",
        "                ToTensor(),\n",
        "                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        )\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        image, _ = self.dataset[idx]\n",
        "        image = image[None, ...]  # add batch dimension\n",
        "        return [image]"
      ],
      "metadata": {
        "id": "Ar581gewwg20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Bit_Compressor(model_path,fp16_mode=True,int8_mode=True,calibration=True,images_folder):\n",
        "  import sys\n",
        "  import torch\n",
        "  import numpy as np\n",
        "  from tqdm.auto import tqdm\n",
        "  assert torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = torch.load(model_path).to(device)\n",
        "  example_input = torch.randn(1, 3, 224, 224).cuda()\n",
        "  #if mode='fp16':\n",
        "    #model_trt = torch2trt(model,[example_input], fp16_mode=True)\n",
        "  #if mode='int8':\n",
        "    #model_trt = torch2trt(model,[example_input], int8_mode=True)\n",
        "  if calibration==True:\n",
        "    dataset = ImageFolderCalibDataset(images_folder)\n",
        "    model_trt = torch2trt(model,[example_input], fp16_mode=fp16_mode,int8_mode=int8_mode,int8_calib_dataset=dataset)\n",
        "  else:\n",
        "    model_trt = torch2trt(model,[example_input], fp16_mode=fp16_mode,int8_mode=int8_mode)\n",
        "  #torch.save(model_trt.state_dict(), model_path+'_quantified')\n",
        "  return model_trt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "v4a60VpKodhL",
        "outputId": "8267ec7c-ffab-4ca4-b5bc-40162fb51829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch2trt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f2faad682bff>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_trt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch2trt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/quantized_classifier.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch2trt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teacher-Student**"
      ],
      "metadata": {
        "id": "-eLYO-cQAqJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "import sys,os\n",
        "import torch, torchvision\n",
        "#import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch import Tensor\n",
        "from functools import partial\n",
        "\n",
        "from torchvision.transforms._presets import ImageClassification\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow"
      ],
      "metadata": {
        "id": "wx5RKToqszgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_classification_path = f'{folder}2. Image classification'\n",
        "\n",
        "training_path=   image_classification_path + '/' + 'data'\n",
        "testing_path = image_classification_path + '/' + 'test'\n",
        "sys.path.append(image_classification_path)\n",
        "os.chdir(image_classification_path)\n",
        "print(image_classification_path)"
      ],
      "metadata": {
        "id": "scDt8dp4s3UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=\"32\" #@param [16,32]\n",
        "batch_size = int(batch_size)\n",
        "validation_split = 0.1\n",
        "seed = 50 #@param"
      ],
      "metadata": {
        "id": "J8hE6Hfjs6y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Evaluation loop\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    verbose=True,\n",
        ") -> float:\n",
        "    model.eval()\n",
        "\n",
        "    num_samples = 0\n",
        "    num_correct = 0\n",
        "    loss = 0\n",
        "\n",
        "    for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False, disable=not verbose):\n",
        "        # Move the data from CPU to GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        # Inference\n",
        "        outputs = model(inputs)\n",
        "        # Calculate loss\n",
        "        loss += F.cross_entropy(outputs, targets, reduction=\"sum\")\n",
        "        # Convert logits to class indices\n",
        "        outputs = outputs.argmax(dim=1)\n",
        "        # Update metrics\n",
        "        num_samples += targets.size(0)\n",
        "        num_correct += (outputs == targets).sum()\n",
        "    return (num_correct / num_samples * 100).item(), (loss / num_samples).item()\n",
        "\n",
        "# training loop\n",
        "def train_kd(\n",
        "    model_student: nn.Module,\n",
        "    path_teacher,\n",
        "    train_loader: DataLoader,\n",
        "    test_loader: DataLoader,\n",
        "    epochs: int,\n",
        "    lr: int,\n",
        "    path,\n",
        "    weight_decay=5e-4,\n",
        "    callbacks=None,\n",
        "    save=None,\n",
        "    save_only_state_dict=False,\n",
        "\n",
        ") -> None:\n",
        "\n",
        "    print('train student model')\n",
        "    model_teacher = torch.load(path_teacher)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model_student.parameters(\n",
        "    ), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40,80], gamma=0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_acc = -1\n",
        "    best_checkpoint = dict()\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model_student.train()\n",
        "        model_teacher.train()\n",
        "        for inputs, targets in tqdm(train_loader, leave=False):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Reset the gradients (from the last iteration)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward inference\n",
        "            out_student = model_student(inputs)\n",
        "            out_teacher = model_teacher(inputs)\n",
        "\n",
        "\n",
        "            # kd loss\n",
        "            kd_T = 4\n",
        "            predict_student = F.log_softmax(out_student / kd_T, dim=1)\n",
        "            predict_teacher = F.softmax(out_teacher / kd_T, dim=1)\n",
        "            alpha = 0.9\n",
        "            loss = nn.KLDivLoss()(predict_student, predict_teacher) * (alpha * kd_T * kd_T) + criterion(out_student, targets) * (1-alpha)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            # Update optimizer\n",
        "            optimizer.step()\n",
        "\n",
        "            if callbacks is not None:\n",
        "                for callback in callbacks:\n",
        "                    callback()\n",
        "\n",
        "        acc, val_loss = evaluate(model_student, test_loader)\n",
        "        print(\n",
        "            f'Epoch {epoch + 1}/{epochs} | Val acc: {acc:.2f} | Val loss: {val_loss:.4f} | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        if best_acc < acc:\n",
        "            best_checkpoint['state_dict'] = copy.deepcopy(model_student.state_dict())\n",
        "            best_acc = acc\n",
        "        # Update LR scheduler\n",
        "        scheduler.step()\n",
        "    model_student.load_state_dict(best_checkpoint['state_dict'])\n",
        "    if save:\n",
        "        # on veut sauvegarder le meilleur modèle\n",
        "        #path = os.path.join(os.getcwd(), \"results\", save)\n",
        "        #os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        #path=os.getcwd()\n",
        "        if save_only_state_dict:\n",
        "            torch.save(model_student.state_dict(), path)\n",
        "        else:\n",
        "            torch.save(model_student, path)\n",
        "    print(f'Best val acc: {best_acc:.2f}')\n",
        "\n",
        "# Fixer le seed pour la reproductibilité\n",
        "import numpy as np\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# train student model\n",
        "def Teacher_Student(student_model,teacher_path,train_dataloader,test_dataloader,epochs=100,lr=0.001,save=True):\n",
        "  print('train student model')\n",
        "  teacher_model = torch.load(teacher_path)\n",
        "  train_kd(student_model, teacher_model, train_dataloader, test_dataloader, epochs, lr, path,save)"
      ],
      "metadata": {
        "id": "aloLCtgyAqhT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBP3xPerSwnC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformView = transforms.Compose([\n",
        "    transforms.Resize(255),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
        "    #transforms.RandomRotation(15),       # Randomly rotate the images by +/- 15 degrees\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(255),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "datasetView = datasets.ImageFolder(training_path, transformView)\n",
        "#dataloaderView = torch.utils.data.DataLoader(datasetView, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.ImageFolder(root=testing_path, transform=test_transforms)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Gz1wxBW8Rjt_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "total_size = len(datasetView)\n",
        "train_size = int(total_size * (1 - validation_split))\n",
        "validation_size = total_size - train_size\n",
        "torch.manual_seed(seed)\n",
        "train_dataset, validation_dataset = random_split(datasetView, [train_size, validation_size])\n",
        "# Create DataLoaders for each dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "B3X4jrPuQPwt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mobilenet(num_classes):\n",
        "    # Load a pre-trained MobileNetV2 model\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "    # Freeze all the parameters in the model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # MobileNetV2 uses 'classifier' instead of 'fc' used by ResNet\n",
        "    # The last layer of MobileNetV2's classifier is a Linear layer with 1280 input features\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "\n",
        "    # Replace the classifier with a new one\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(num_ftrs, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(128, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(32, num_classes),\n",
        "        nn.Softmax(dim=1)  # Softmax for classification\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "2qOugMTAV73u"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = create_mobilenet(3)\n",
        "student_model.to(device)\n",
        "path_teacher=image_classification_path + '/' + 'model-Resnet-Epoch10.pth'\n",
        "#student_model=models.mobilenet_v2(pretrained=True)\n",
        "epochs=10\n",
        "#f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\"\n",
        "path=classification_path + '/' + f\"student_mobilenet_e{epochs}_resnet_e10\"\n",
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMU0qjA9TZnH",
        "outputId": "63e2886b-c9c9-4864-ffaa-79e68d9b955e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IA/UMONS/Workshop/2. Image classification//student_mobilenet_e10_resnet_e10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_kd(student_model, path_teacher, train_loader, validation_loader, epochs=epochs, lr=0.01, path=path,save=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNfpC9_jlsSV",
        "outputId": "c0f1d7ee-2725-49e8-9f68-ac19c0f35bff"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train student model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Val acc: 50.71 | Val loss: 1.0116 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 | Val acc: 84.36 | Val loss: 0.7875 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 | Val acc: 84.83 | Val loss: 0.7107 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 | Val acc: 88.63 | Val loss: 0.6721 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 | Val acc: 89.57 | Val loss: 0.6599 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 | Val acc: 89.57 | Val loss: 0.6586 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 | Val acc: 90.52 | Val loss: 0.6484 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 | Val acc: 91.00 | Val loss: 0.6490 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 | Val acc: 91.94 | Val loss: 0.6351 | LR: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 | Val acc: 90.52 | Val loss: 0.6409 | LR: 0.010000\n",
            "Best val acc: 91.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients for evaluation\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total * 100\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "k3gYwTrbZem3"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = torch.load(path)"
      ],
      "metadata": {
        "id": "9BFK9ZqhaUx8"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate_model(student_model, train_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "kNOOSNc2ZkAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_loss, test_accuracy = evaluate_model(student_model, validation_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmx43UF0q-TL",
        "outputId": "993db299-51ac-4c3a-c7ec-9f948da4101b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6415, Test Accuracy: 91.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_loss, test_accuracy = evaluate_model(student_model, train_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5AzI1idr6Rh",
        "outputId": "4a00a07e-099c-42ec-d7fe-18270be8571c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6439, Test Accuracy: 90.54%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "0HqpyiKFTqRH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}