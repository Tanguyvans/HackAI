{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":969357,"sourceType":"datasetVersion","datasetId":529007}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span> ðŸ”¥ðŸ”¥ðŸ”¥ Fire detection using Vision Transformers (ViT) </span>\n<hr style=\"border-bottom: solid;background-color:light;color:black;\">","metadata":{}},{"cell_type":"markdown","source":"<h2>Introduction </h2>  \n<p style=\"text-align:justify; padding:20px;\">\nIn this notebook we will talk about classification & Transformers. We fine tune a Tranformer model to detect fire.\n</p>\n\n* [Imports](#section-1)\n* [Data preparation](#section-2)\n* [Building the model](#section-3)\n* [Training the model](#section-4)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T08:50:22.368612Z","iopub.execute_input":"2022-10-30T08:50:22.369059Z","iopub.status.idle":"2022-10-30T08:50:22.378140Z","shell.execute_reply.started":"2022-10-30T08:50:22.369024Z","shell.execute_reply":"2022-10-30T08:50:22.376344Z"}}},{"cell_type":"markdown","source":"<a id=\"section-1\"></a>\n# <span>1. Imports</span>\n<hr style=\"border-bottom: solid;background-color:light;color:black;\">","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import load_metric\n\nfrom transformers import TrainingArguments\nfrom transformers import ViTFeatureExtractor\nfrom transformers import ViTForImageClassification\n\nimport torch\n\nfrom PIL import Image\nimport requests\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:07.804272Z","iopub.execute_input":"2024-05-19T12:26:07.805190Z","iopub.status.idle":"2024-05-19T12:26:15.461792Z","shell.execute_reply.started":"2024-05-19T12:26:07.805094Z","shell.execute_reply":"2024-05-19T12:26:15.460384Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-2\"></a>\n# <span>2. Data preparation</span>\n<hr style=\"border-bottom: solid;background-color:light;color:black;\">","metadata":{}},{"cell_type":"code","source":"ds = load_dataset(\"imagefolder\", data_dir = \"../input/fire-dataset/fire_dataset\")\nds","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:15.463841Z","iopub.execute_input":"2024-05-19T12:26:15.464765Z","iopub.status.idle":"2024-05-19T12:26:28.478074Z","shell.execute_reply.started":"2024-05-19T12:26:15.464732Z","shell.execute_reply":"2024-05-19T12:26:28.477029Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/999 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7414defdd2144250b1dd2f40b6681166"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset image_folder/default to /root/.cache/huggingface/datasets/image_folder/default-205668c41abe4b96/0.0.0/ee92df8e96c6907f3c851a987be3fd03d4b93b247e727b69a8e23ac94392a091...\n                ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files #2:   0%|          | 0/63 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec93c39b11e4798bf74b639e22032c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #4:   0%|          | 0/63 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7792a23662473889f9d50c0c6399d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #11:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2b358e1e23a4d6087748d1397021772"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #1:   0%|          | 0/63 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c1749b040e4bcebf40e45093c08853"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #0:   0%|          | 0/63 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2742339b3b6b4f91a4f1547dcdd8ba87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #14:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53894ee498c04258888e99c1313c64d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #3:   0%|          | 0/63 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d719d403abe748cc85240f8919fdeccc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #9:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a4def54b1c04ba78d196f7540ef813c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #12:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d3fa5dba8141a58f1d948e6893c409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #5:   0%|          | 0/63 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25b357009e634f35a25e20baec955499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #7:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6da4976f3842a48914f8d3c0f45811"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #13:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ee4db64f5f4945bb2d75c11c5fcfad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #8:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"184b04dcf02d48bcb4108f23cc3e4b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #6:   0%|          | 0/63 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b3b0b7feed4c9f9ff943f72a6c3e63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #10:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"536ca3d21b364d7499368e3510c7eb4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files #15:   0%|          | 0/62 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fca666e0ce24568b6dea31bdb9bd1e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"134e5da9d96b4090b2c946f772feab59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9453b4abff4646a25a4fa97dc38a26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset image_folder downloaded and prepared to /root/.cache/huggingface/datasets/image_folder/default-205668c41abe4b96/0.0.0/ee92df8e96c6907f3c851a987be3fd03d4b93b247e727b69a8e23ac94392a091. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae2e9558540a4c688f9980ea961c9089"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 999\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"data = ds['train'].train_test_split(test_size = 0.1)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:28.479692Z","iopub.execute_input":"2024-05-19T12:26:28.480012Z","iopub.status.idle":"2024-05-19T12:26:28.532910Z","shell.execute_reply.started":"2024-05-19T12:26:28.479984Z","shell.execute_reply":"2024-05-19T12:26:28.531839Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 899\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"labels = data['train'].features['label']\nlabels = data[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:28.535928Z","iopub.execute_input":"2024-05-19T12:26:28.536328Z","iopub.status.idle":"2024-05-19T12:26:28.542411Z","shell.execute_reply.started":"2024-05-19T12:26:28.536290Z","shell.execute_reply":"2024-05-19T12:26:28.541304Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"metric = load_metric('accuracy')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:28.543761Z","iopub.execute_input":"2024-05-19T12:26:28.544043Z","iopub.status.idle":"2024-05-19T12:26:29.662209Z","shell.execute_reply.started":"2024-05-19T12:26:28.544017Z","shell.execute_reply":"2024-05-19T12:26:29.661380Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31f90f7f7e3f4a43a2bd0b257e2a1476"}},"metadata":{}}]},{"cell_type":"code","source":" feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:29.663395Z","iopub.execute_input":"2024-05-19T12:26:29.663710Z","iopub.status.idle":"2024-05-19T12:26:29.973538Z","shell.execute_reply.started":"2024-05-19T12:26:29.663681Z","shell.execute_reply":"2024-05-19T12:26:29.972582Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d07f83d3369546dbaf1df9af939be2dd"}},"metadata":{}}]},{"cell_type":"code","source":"from torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize,\n    ToTensor,\n)\n\nnormalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\ntrain_transforms = Compose(\n        [\n            RandomResizedCrop(feature_extractor.size),\n            RandomHorizontalFlip(),\n            ToTensor(),\n            normalize,\n        ]\n    )\n\nval_transforms = Compose(\n        [\n            Resize(feature_extractor.size),\n            CenterCrop(feature_extractor.size),\n            ToTensor(),\n            normalize,\n        ]\n    )\n\ndef preprocess_train(example_batch):\n    \"\"\"Apply train_transforms across a batch.\"\"\"\n    example_batch[\"pixel_values\"] = [\n        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n    ]\n    return example_batch\n\ndef preprocess_val(example_batch):\n    \"\"\"Apply val_transforms across a batch.\"\"\"\n    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n    return example_batch","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:29.975050Z","iopub.execute_input":"2024-05-19T12:26:29.975692Z","iopub.status.idle":"2024-05-19T12:26:30.191328Z","shell.execute_reply.started":"2024-05-19T12:26:29.975650Z","shell.execute_reply":"2024-05-19T12:26:30.190162Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_ds = data['train']\nval_ds = data['test']\ntest_ds = data['test']","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:30.192785Z","iopub.execute_input":"2024-05-19T12:26:30.193078Z","iopub.status.idle":"2024-05-19T12:26:30.198082Z","shell.execute_reply.started":"2024-05-19T12:26:30.193052Z","shell.execute_reply":"2024-05-19T12:26:30.197082Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:30.199417Z","iopub.execute_input":"2024-05-19T12:26:30.199787Z","iopub.status.idle":"2024-05-19T12:26:30.209705Z","shell.execute_reply.started":"2024-05-19T12:26:30.199760Z","shell.execute_reply":"2024-05-19T12:26:30.208815Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:30.213430Z","iopub.execute_input":"2024-05-19T12:26:30.213870Z","iopub.status.idle":"2024-05-19T12:26:30.281459Z","shell.execute_reply.started":"2024-05-19T12:26:30.213832Z","shell.execute_reply":"2024-05-19T12:26:30.280371Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=448x300>,\n 'label': 0,\n 'pixel_values': tensor([[[-0.5059, -0.4824, -0.6235,  ..., -0.4902, -0.4118, -0.3961],\n          [-0.6549, -0.5686, -0.5216,  ..., -0.3490, -0.3490, -0.4353],\n          [-0.6549, -0.6471, -0.5451,  ..., -0.4039, -0.3255, -0.3647],\n          ...,\n          [ 0.3804,  0.3804,  0.0588,  ..., -0.2471, -0.2392, -0.2314],\n          [ 0.8196,  0.6627,  0.1922,  ..., -0.3490, -0.3725, -0.3569],\n          [ 0.9922,  0.7569,  0.4196,  ..., -0.4275, -0.4667, -0.4431]],\n \n         [[-0.5451, -0.5216, -0.6627,  ..., -0.4118, -0.3412, -0.3255],\n          [-0.7098, -0.6235, -0.5765,  ..., -0.2706, -0.2784, -0.3647],\n          [-0.7098, -0.7020, -0.6000,  ..., -0.3255, -0.2549, -0.3020],\n          ...,\n          [-0.4039, -0.3490, -0.6471,  ..., -0.5686, -0.6078, -0.6314],\n          [ 0.0510, -0.0667, -0.5216,  ..., -0.5294, -0.5765, -0.5765],\n          [ 0.2392, -0.0353, -0.3490,  ..., -0.6549, -0.6941, -0.6549]],\n \n         [[-0.5922, -0.5686, -0.7098,  ..., -0.4745, -0.3804, -0.3333],\n          [-0.7647, -0.6784, -0.6314,  ..., -0.3333, -0.3020, -0.3725],\n          [-0.7725, -0.7647, -0.6627,  ..., -0.3725, -0.2784, -0.2941],\n          ...,\n          [-0.9216, -0.8745, -0.9373,  ..., -0.6941, -0.7176, -0.7176],\n          [-0.7569, -0.8118, -1.0000,  ..., -0.5843, -0.6078, -0.5765],\n          [-0.7098, -0.9451, -0.9843,  ..., -0.7176, -0.7255, -0.7098]]])}"},"metadata":{}}]},{"cell_type":"code","source":"model_name_or_path = 'google/vit-base-patch16-224-in21k'\nmodel = ViTForImageClassification.from_pretrained(\n    model_name_or_path, \n    num_labels=len(labels),\n    id2label={str(i): c for i, c in enumerate(labels)},\n    label2id={c: str(i) for i, c in enumerate(labels)}\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:30.282995Z","iopub.execute_input":"2024-05-19T12:26:30.283582Z","iopub.status.idle":"2024-05-19T12:26:41.874484Z","shell.execute_reply.started":"2024-05-19T12:26:30.283539Z","shell.execute_reply":"2024-05-19T12:26:41.873656Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f096f259837b47a6bd3d16d02c28671c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/330M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a24bb14082146dba03e06c337e09708"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"section-3\"></a>\n# <span>3. Building the model</span>\n<hr style=\"border-bottom: solid;background-color:light;color:black;\">","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    'finetuned-fire-detection',\n  per_device_train_batch_size=16,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=4,\n  fp16=True,\n  save_steps=100,\n  eval_steps=100,\n  logging_steps=10,\n  learning_rate=2e-4,\n  save_total_limit=2,\n  remove_unused_columns=False,\n  report_to='tensorboard',\n  load_best_model_at_end=True,\n  hub_strategy=\"end\"\n)#","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:41.875985Z","iopub.execute_input":"2024-05-19T12:26:41.876614Z","iopub.status.idle":"2024-05-19T12:26:41.936251Z","shell.execute_reply.started":"2024-05-19T12:26:41.876570Z","shell.execute_reply":"2024-05-19T12:26:41.935185Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:41.937544Z","iopub.execute_input":"2024-05-19T12:26:41.937884Z","iopub.status.idle":"2024-05-19T12:26:41.943404Z","shell.execute_reply.started":"2024-05-19T12:26:41.937856Z","shell.execute_reply":"2024-05-19T12:26:41.942365Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['label'] for x in batch])\n    }","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:41.944765Z","iopub.execute_input":"2024-05-19T12:26:41.945586Z","iopub.status.idle":"2024-05-19T12:26:41.954212Z","shell.execute_reply.started":"2024-05-19T12:26:41.945549Z","shell.execute_reply":"2024-05-19T12:26:41.953364Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n    data_collator=collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:41.955328Z","iopub.execute_input":"2024-05-19T12:26:41.955642Z","iopub.status.idle":"2024-05-19T12:26:46.742394Z","shell.execute_reply.started":"2024-05-19T12:26:41.955600Z","shell.execute_reply":"2024-05-19T12:26:46.741585Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Using cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"section-4\"></a>\n# <span>4. Training the model</span>\n<hr style=\"border-bottom: solid;background-color:light;color:black;\">","metadata":{}},{"cell_type":"code","source":"train_results = trainer.train()\n# rest is optional but nice to have\ntorch.save(model, \"model.pth\")\n#trainer.save_model('model.h5')\n#trainer.log_metrics(\"train\", train_results.metrics)\n#trainer.save_metrics(\"train\", train_results.metrics)\n#trainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:26:46.743533Z","iopub.execute_input":"2024-05-19T12:26:46.744192Z","iopub.status.idle":"2024-05-19T12:30:29.307513Z","shell.execute_reply.started":"2024-05-19T12:26:46.744155Z","shell.execute_reply":"2024-05-19T12:30:29.306637Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 899\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 228\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [228/228 03:35, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.036000</td>\n      <td>0.010521</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.077000</td>\n      <td>0.006834</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 8\nSaving model checkpoint to finetuned-fire-detection/checkpoint-100\nConfiguration saved in finetuned-fire-detection/checkpoint-100/config.json\nModel weights saved in finetuned-fire-detection/checkpoint-100/pytorch_model.bin\nFeature extractor saved in finetuned-fire-detection/checkpoint-100/preprocessor_config.json\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 8\nSaving model checkpoint to finetuned-fire-detection/checkpoint-200\nConfiguration saved in finetuned-fire-detection/checkpoint-200/config.json\nModel weights saved in finetuned-fire-detection/checkpoint-200/pytorch_model.bin\nFeature extractor saved in finetuned-fire-detection/checkpoint-200/preprocessor_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from finetuned-fire-detection/checkpoint-200 (score: 0.006833572406321764).\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, \"model.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:30:29.308868Z","iopub.execute_input":"2024-05-19T12:30:29.309199Z","iopub.status.idle":"2024-05-19T12:30:30.117849Z","shell.execute_reply.started":"2024-05-19T12:30:29.309163Z","shell.execute_reply":"2024-05-19T12:30:30.116685Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = torch.load(\"model.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:30:30.119474Z","iopub.execute_input":"2024-05-19T12:30:30.120128Z","iopub.status.idle":"2024-05-19T12:30:30.308267Z","shell.execute_reply.started":"2024-05-19T12:30:30.120085Z","shell.execute_reply":"2024-05-19T12:30:30.307274Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"metrics = trainer.evaluate()\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:30:30.309549Z","iopub.execute_input":"2024-05-19T12:30:30.309885Z","iopub.status.idle":"2024-05-19T12:30:35.302016Z","shell.execute_reply.started":"2024-05-19T12:30:30.309855Z","shell.execute_reply":"2024-05-19T12:30:35.300979Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =        4.0\n  eval_accuracy           =        1.0\n  eval_loss               =     0.0068\n  eval_runtime            = 0:00:04.73\n  eval_samples_per_second =     21.109\n  eval_steps_per_second   =      2.744\n","output_type":"stream"}]},{"cell_type":"code","source":"outputs = trainer.predict(test_ds)\nprint(outputs.metrics)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:30:35.303276Z","iopub.execute_input":"2024-05-19T12:30:35.303665Z","iopub.status.idle":"2024-05-19T12:30:40.053777Z","shell.execute_reply.started":"2024-05-19T12:30:35.303608Z","shell.execute_reply":"2024-05-19T12:30:40.052771Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 100\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"{'test_loss': 0.006833572406321764, 'test_accuracy': 1.0, 'test_runtime': 4.7439, 'test_samples_per_second': 21.08, 'test_steps_per_second': 2.74}\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.is_available = lambda : False\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nurl = '../input/fire-dataset/fire_dataset/non_fire_images/non_fire.11.png'\n\nimage = Image.open(url)\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", id2label[predicted_class_idx])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:30:40.054839Z","iopub.execute_input":"2024-05-19T12:30:40.055112Z","iopub.status.idle":"2024-05-19T12:30:40.583619Z","shell.execute_reply.started":"2024-05-19T12:30:40.055086Z","shell.execute_reply":"2024-05-19T12:30:40.582605Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Predicted class: non_fire_images\n","output_type":"stream"}]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:30:40.585029Z","iopub.execute_input":"2024-05-19T12:30:40.585415Z","iopub.status.idle":"2024-05-19T12:30:41.616869Z","shell.execute_reply.started":"2024-05-19T12:30:40.585373Z","shell.execute_reply":"2024-05-19T12:30:41.615026Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mfinetuned-fire-detection\u001b[0m/  model.pth\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a href=\"model.pth\"> Download File </a>","metadata":{}},{"cell_type":"code","source":"model_path = '/kaggle/working/model.pth'","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:30:41.618960Z","iopub.execute_input":"2024-05-19T12:30:41.620190Z","iopub.status.idle":"2024-05-19T12:30:41.626259Z","shell.execute_reply.started":"2024-05-19T12:30:41.620146Z","shell.execute_reply":"2024-05-19T12:30:41.624982Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = torch.load(\"model.pth\", map_location=torch.device('cpu'))\n#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#model = model.to(device)\n\nurl = '../input/fire-dataset/fire_dataset/non_fire_images/non_fire.11.png'\n\nimage = Image.open(url)\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", id2label[predicted_class_idx])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:34:38.897618Z","iopub.execute_input":"2024-05-19T12:34:38.898569Z","iopub.status.idle":"2024-05-19T12:34:39.475787Z","shell.execute_reply.started":"2024-05-19T12:34:38.898530Z","shell.execute_reply":"2024-05-19T12:34:39.474641Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Predicted class: non_fire_images\n","output_type":"stream"}]},{"cell_type":"code","source":"logits.argmax(-1).item()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:08:54.083734Z","iopub.execute_input":"2024-05-19T13:08:54.084171Z","iopub.status.idle":"2024-05-19T13:08:54.091353Z","shell.execute_reply.started":"2024-05-19T13:08:54.084129Z","shell.execute_reply":"2024-05-19T13:08:54.090440Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"import pkg_resources\nimport types\ndef get_imports():\n    for name, val in globals().items():\n        if isinstance(val, types.ModuleType):\n            # Split ensures you get root package, \n            # not just imported function\n            name = val.__name__.split(\".\")[0]\n\n        elif isinstance(val, type):\n            name = val.__module__.split(\".\")[0]\n\n        # Some packages are weird and have different\n        # imported names vs. system names\n        if name == \"PIL\":\n            name = \"Pillow\"\n        elif name == \"sklearn\":\n            name = \"scikit-learn\"\n\n        yield name\nimports = list(set(get_imports()))\n\nrequirements = []\nfor m in pkg_resources.working_set:\n    if m.project_name in imports and m.project_name!=\"pip\":\n        requirements.append((m.project_name, m.version))\n\nfor r in requirements:\n    print(\"{}=={}\".format(*r))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T12:36:07.907212Z","iopub.execute_input":"2024-05-19T12:36:07.908261Z","iopub.status.idle":"2024-05-19T12:36:07.920649Z","shell.execute_reply.started":"2024-05-19T12:36:07.908222Z","shell.execute_reply":"2024-05-19T12:36:07.919667Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Pillow==9.1.1\ntransformers==4.20.1\nrequests==2.28.1\nnumpy==1.21.6\ntorch==1.11.0\ntorchvision==0.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}